{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "serp_api = '2db975c7f72d37a545504fb166033d19ccbadcf5eeaf17b56882f9a81d0dafe7'\n",
    "cohere_api = '6rIWhaa4vGVQKRnSJ9ClSKkgsxQp8EabiXBLH2zl'\n",
    "elsevier_apikey = 'ce04ffa20c7ee4560e4ffd4761bbe157'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import bs4\n",
    "import time\n",
    "import lxml\n",
    "import cohere\n",
    "from cohere.classify import Example \n",
    "import pandas as pd\n",
    "from unidecode import unidecode\n",
    "import json\n",
    "from qa.search import embedding_search, get_results_paragraphs_multi_process, get_results_paragraphs_from_paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class paper_parser():\n",
    "    def __init__(self, query, apikey, count, start, sort, view, timeout):\n",
    "        self.query = query\n",
    "        self.apikey = apikey\n",
    "        self.count = count\n",
    "        self.start = start\n",
    "        self.sort = sort\n",
    "        self.view = view\n",
    "        self.timeout = timeout\n",
    "        self.dic = self.search_for_papers(self.query, self.apikey, self.count, self.start, self.sort, self.view)\n",
    "        self.piis = list(self.dic.values())\n",
    "        self.paragraphs = self.get_all_paragraphs()\n",
    "        self.titles = self.get_all_titles()\n",
    "        self.authors = self.get_all_1st_authors()\n",
    "        self.organizations = self.get_all_organizations()\n",
    "    def search_for_papers(self, query, apikey, count, start, sort, view):\n",
    "        url = 'https://api.elsevier.com/content/search/sciencedirect?'\n",
    "        headers = {'X-ELS-APIKey': apikey, 'Accept': 'application/json'}\n",
    "        params = {'query': query, 'count': count, 'start': start, 'sort': sort, 'view': view}\n",
    "        r = requests.get(url, headers=headers, params=params)\n",
    "        json_resp = json.loads(r.text)\n",
    "        titles, piis, authors = [], [], []\n",
    "        dic = {}\n",
    "        for paper in json_resp['search-results']['entry']:\n",
    "            titles.append(paper['dc:title'])\n",
    "            piis.append(paper['pii'])\n",
    "            dic[paper['dc:title']] = paper['pii']\n",
    "        return dic\n",
    "    def get_paragraphs(self, pii):\n",
    "        url = f'https://api.elsevier.com/content/article/pii/{pii}'\n",
    "        response = requests.get(url, headers={\"X-ELS-APIKey\":self.apikey, \"content-type\": \"text/xml;charset=UTF-8\"}, timeout = self.timeout)\n",
    "        soup = BeautifulSoup(response.content, \"xml\")\n",
    "        text =[unidecode(''.join(s.findAll(text=True)).replace('\\n', '')) for s in soup.find_all('ce:para')]\n",
    "        return text\n",
    "    def get_all_1st_authors(self):\n",
    "        authors = []\n",
    "        for pii in self.piis:\n",
    "            url = f'https://api.elsevier.com/content/article/pii/{pii}'\n",
    "            response = requests.get(url, headers={\"X-ELS-APIKey\":self.apikey, \"content-type\": \"text/xml;charset=UTF-8\"}, timeout = self.timeout)\n",
    "            soup = BeautifulSoup(response.content, \"xml\")\n",
    "            first_name = [''.join(s.findAll(text=True)) for s in soup.find_all('ce:given-name')][0]\n",
    "            last_name = [''.join(s.findAll(text=True)) for s in soup.find_all('ce:surname')][0]\n",
    "            name = first_name + ' ' + last_name\n",
    "            authors.append(name)\n",
    "        return authors\n",
    "    def get_all_organizations(self):\n",
    "        organizations = []\n",
    "        for pii in self.piis:\n",
    "            url = f'https://api.elsevier.com/content/article/pii/{pii}'\n",
    "            response = requests.get(url, headers={\"X-ELS-APIKey\":self.apikey, \"content-type\": \"text/xml;charset=UTF-8\"}, timeout = self.timeout)\n",
    "            soup = BeautifulSoup(response.content, \"xml\")            \n",
    "            organization = [''.join(s.findAll(text=True)) for s in soup.find_all('sa:organization')]\n",
    "            # join all organizations into one string and split by comma\n",
    "            organization = ', '.join(organization)\n",
    "            organizations.append(organization)\n",
    "        return organizations\n",
    "    def get_all_paragraphs(self):\n",
    "        paragraphs = []\n",
    "        for pii in self.piis:\n",
    "            paragraphs.append(self.get_paragraphs(pii))\n",
    "        return paragraphs\n",
    "    def get_all_titles(self):\n",
    "        return list(self.dic.keys())\n",
    "    def get_text(self, pii, apikey, timeout):\n",
    "        url = f'https://api.elsevier.com/content/article/pii/{pii}'\n",
    "        response = requests.get(url, headers={\"X-ELS-APIKey\":apikey, \"content-type\": \"text/xml;charset=UTF-8\"}, timeout = timeout)\n",
    "        soup = BeautifulSoup(response.content, \"xml\")\n",
    "        return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp = paper_parser('Machine learning assisted design of FeCoNiCrMn high-entropy alloys with ultra-low hydrogen diffusion coefficients', elsevier_apikey, 1, 0, 'relevance', 'COMPLETE', 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_papers = pp.get_all_paragraphs()\n",
    "example_titles = pp.get_all_titles()\n",
    "example_authors = pp.get_all_1st_authors()\n",
    "example_organizations = pp.get_all_organizations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['S1359645421009137']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pp.piis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to a csv file\n",
    "df = pd.DataFrame(example_papers[0], columns = ['paragraphs'])\n",
    "df['titles'] = example_titles[0]\n",
    "df['authors'] = example_authors[0]\n",
    "df['organizations'] = example_organizations[0]\n",
    "# df.to_csv('S1359645421009137.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relevant_paper_from_question(question):\n",
    "    co = cohere.Client('6rIWhaa4vGVQKRnSJ9ClSKkgsxQp8EabiXBLH2zl') \n",
    "    response = co.generate( \n",
    "        model='command-xlarge-20221108', \n",
    "        prompt='Extract the key words and seperate them by commas from the following question:\\n\\nQuestion: '+ question +'\\n\\nKey words:', \n",
    "        max_tokens=50, \n",
    "        temperature=0.6, \n",
    "        k=0, \n",
    "        p=1, \n",
    "        frequency_penalty=0, \n",
    "        presence_penalty=0, \n",
    "        stop_sequences=[\"--\"], \n",
    "        return_likelihoods='NONE') \n",
    "    key_words = response.generations[0].text\n",
    "    print(key_words)\n",
    "    pp = paper_parser(key_words, elsevier_apikey, 1, 0, 'relevance', 'COMPLETE', 60)\n",
    "    paragraphs = []\n",
    "    paragraph_sources = []\n",
    "    for i in range(len(pp.get_all_paragraphs()[0])):\n",
    "        paragraphs += pp.get_all_paragraphs()[0][i]\n",
    "        paragraph_sources += [pp.piis(0)]  * len(url_paragraphs[i])\n",
    "    return paragraphs, paragraph_sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "NIPS, membrane, synthesis\n"
     ]
    }
   ],
   "source": [
    "get_relevant_paper_from_question('What is the best way to synthesize a membrane with NIPS?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('S1359645421009137'+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(get_results_paragraphs_from_paper('S1359645421009137')[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "178729c8f5e9eedf2bae7ea816478a89001acb4e6c66f13ce64ddbee9dd2f878"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
